{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raport 2: Implementacja Podstawowa\n",
    "## Detekcja Anomalii - LOF i PCA\n",
    "\n",
    "**Projekt 7**: Anomalia i uczenie maszynowe  \n",
    "**Rok akademicki**: 2025/2026\n",
    "\n",
    "---\n",
    "\n",
    "Ten notebook demonstruje podstawową implementację algorytmów:\n",
    "1. **LOF** (Local Outlier Factor)\n",
    "2. **PCA** (Principal Component Analysis) dla detekcji anomalii\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from src.algorithms.lof import LOF\n",
    "from src.algorithms.pca_anomaly import PCAAnomaly\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Local Outlier Factor (LOF)\n",
    "\n",
    "### 1.1 Algorytm\n",
    "\n",
    "LOF identyfikuje anomalie lokalne poprzez porównanie gęstości punktu z gęstością jego sąsiadów.\n",
    "\n",
    "**Kluczowe koncepcje**:\n",
    "- **k-distance**: odległość do k-tego najbliższego sąsiada\n",
    "- **Reachability distance**: `reach-dist(p, o) = max(k-distance(o), d(p, o))`\n",
    "- **Local Reachability Density** (LRD): odwrotność średniej reachability distance\n",
    "- **LOF score**: `LOF(p) = średnia(LRD sąsiadów) / LRD(p)`\n",
    "\n",
    "**Interpretacja**:\n",
    "- LOF ≈ 1: punkt normalny\n",
    "- LOF > 1: potencjalna anomalia (niższa gęstość niż sąsiedzi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Prosty przykład 2D z jednym outlierem\n",
    "print(\"Test 1: LOF na prostych danych 2D\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create data: cluster + 1 outlier\n",
    "X_cluster = np.array([\n",
    "    [0, 0],\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0.5, 0.5],\n",
    "    [1, 0.5],\n",
    "    [0.5, 1]\n",
    "])\n",
    "X_outlier = np.array([[5, 5]])\n",
    "X = np.vstack([X_cluster, X_outlier])\n",
    "\n",
    "# Fit LOF\n",
    "lof = LOF(n_neighbors=3)\n",
    "scores = lof.fit_predict(X)\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"\\nLOF scores:\")\n",
    "for i, score in enumerate(scores):\n",
    "    label = \"OUTLIER\" if score > 1.5 else \"NORMAL\"\n",
    "    print(f\"  Point {i}: LOF = {score:.3f} [{label}]\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "# Plot points\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], c=scores, s=200, \n",
    "                    cmap='RdYlGn_r', edgecolors='black', linewidths=2)\n",
    "\n",
    "# Annotate with LOF scores\n",
    "for i, (x, y) in enumerate(X):\n",
    "    ax.annotate(f'{scores[i]:.2f}', (x, y), \n",
    "               textcoords=\"offset points\", xytext=(0,10), \n",
    "               ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12)\n",
    "ax.set_title('LOF: Simple 2D Example (k=3)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='LOF Score')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Gaussowskie klastry z outlierami\n",
    "print(\"\\nTest 2: LOF na danych gaussowskich\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate Gaussian cluster\n",
    "X_inliers = np.random.randn(100, 2) * 0.5\n",
    "\n",
    "# Add outliers\n",
    "X_outliers = np.array([\n",
    "    [3, 3],\n",
    "    [-3, 3],\n",
    "    [3, -3],\n",
    "    [-3, -2.5]\n",
    "])\n",
    "\n",
    "X = np.vstack([X_inliers, X_outliers])\n",
    "y_true = np.hstack([np.zeros(100), np.ones(4)])  # Ground truth labels\n",
    "\n",
    "# Fit LOF\n",
    "lof = LOF(n_neighbors=20)\n",
    "scores = lof.fit_predict(X)\n",
    "\n",
    "# Threshold\n",
    "threshold = 1.5\n",
    "y_pred = (scores > threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nMetrics (threshold={threshold}):\")\n",
    "print(f\"  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall: {recall:.3f}\")\n",
    "print(f\"  F1-score: {f1:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: LOF scores\n",
    "scatter1 = axes[0].scatter(X[:, 0], X[:, 1], c=scores, s=50, \n",
    "                          cmap='RdYlGn_r', edgecolors='black', linewidths=0.5)\n",
    "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0].set_title('LOF Scores (k=20)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='LOF Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Predictions vs Ground Truth\n",
    "axes[1].scatter(X[y_true == 0, 0], X[y_true == 0, 1], \n",
    "               c='blue', s=50, alpha=0.6, label='True Inliers', edgecolors='black', linewidths=0.5)\n",
    "axes[1].scatter(X[y_true == 1, 0], X[y_true == 1, 1], \n",
    "               c='red', s=200, marker='*', label='True Outliers', edgecolors='black', linewidths=1)\n",
    "axes[1].scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], \n",
    "               facecolors='none', s=300, marker='o', \n",
    "               edgecolors='orange', linewidths=3, label='Detected Outliers')\n",
    "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[1].set_title(f'Detection Results (F1={f1:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Wpływ parametru k\n",
    "print(\"\\nTest 3: Wpływ parametru k na wyniki LOF\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "k_values = [5, 10, 20, 30]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, k in enumerate(k_values):\n",
    "    lof = LOF(n_neighbors=k)\n",
    "    scores = lof.fit_predict(X)\n",
    "    \n",
    "    scatter = axes[idx].scatter(X[:, 0], X[:, 1], c=scores, s=50, \n",
    "                               cmap='RdYlGn_r', edgecolors='black', linewidths=0.5)\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=10)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=10)\n",
    "    axes[idx].set_title(f'LOF with k={k}', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(scatter, ax=axes[idx], label='LOF Score')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserwacje:\")\n",
    "print(\"  - Małe k: bardziej wrażliwe na anomalie lokalne\")\n",
    "print(\"  - Duże k: wykrywa anomalie globalne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis (PCA)\n",
    "\n",
    "### 2.1 Algorytm\n",
    "\n",
    "PCA redukuje wymiarowość danych poprzez projekcję na kierunki maksymalnej wariancji.\n",
    "\n",
    "**Kroki algorytmu**:\n",
    "1. Standaryzacja danych\n",
    "2. Obliczenie macierzy kowariancji\n",
    "3. Wyznaczenie wektorów własnych (składowych głównych)\n",
    "4. Projekcja danych na PC\n",
    "\n",
    "**Detekcja anomalii**:\n",
    "- **Reconstruction Error**: `||x - x_reconstructed||²`\n",
    "- **Mahalanobis Distance**: odległość w przestrzeni PC z uwzględnieniem wariancji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: PCA - Reconstruction Error\n",
    "print(\"\\nTest 4: PCA - Reconstruction Error Method\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create data along main axis with outlier perpendicular\n",
    "np.random.seed(42)\n",
    "X_line = np.column_stack([\n",
    "    np.linspace(0, 10, 30),\n",
    "    np.linspace(0, 10, 30) + np.random.randn(30) * 0.3\n",
    "])\n",
    "X_outliers_pca = np.array([\n",
    "    [5, -2],\n",
    "    [2, 8]\n",
    "])\n",
    "X_pca = np.vstack([X_line, X_outliers_pca])\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCAAnomaly(n_components=1, method='reconstruction')\n",
    "pca.fit(X_pca)\n",
    "\n",
    "# Get reconstruction errors\n",
    "errors = pca.reconstruction_error(X_pca)\n",
    "scores = pca.score_samples(X_pca)\n",
    "\n",
    "print(f\"\\nExplained variance ratio: {pca.explained_variance_ratio_[0]:.3f}\")\n",
    "print(f\"Mean reconstruction error (inliers): {np.mean(errors[:30]):.4f}\")\n",
    "print(f\"Reconstruction errors (outliers): {errors[30:]}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Data with PC direction\n",
    "axes[0].scatter(X_pca[:30, 0], X_pca[:30, 1], c='blue', s=50, \n",
    "               alpha=0.6, label='Inliers', edgecolors='black', linewidths=0.5)\n",
    "axes[0].scatter(X_pca[30:, 0], X_pca[30:, 1], c='red', s=200, \n",
    "               marker='*', label='Outliers', edgecolors='black', linewidths=1)\n",
    "\n",
    "# Plot principal component\n",
    "mean = pca.mean_\n",
    "pc = pca.components_[0] * pca.std_  # Unstandardize for plotting\n",
    "axes[0].arrow(mean[0], mean[1], pc[0]*3, pc[1]*3, \n",
    "             head_width=0.3, head_length=0.3, fc='green', ec='green', linewidth=3)\n",
    "axes[0].plot([mean[0] - pc[0]*3, mean[0] + pc[0]*3],\n",
    "            [mean[1] - pc[1]*3, mean[1] + pc[1]*3],\n",
    "            'g--', linewidth=2, label='1st Principal Component', alpha=0.7)\n",
    "\n",
    "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0].set_title('PCA: Data and Principal Component', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Reconstruction errors\n",
    "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=errors, s=100, \n",
    "                          cmap='RdYlGn_r', edgecolors='black', linewidths=1)\n",
    "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[1].set_title('Reconstruction Error', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Reconstruction Error')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: PCA - Explained Variance\n",
    "print(\"\\nTest 5: PCA - Explained Variance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate 5D data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Create correlated features\n",
    "z1 = np.random.randn(n_samples)\n",
    "z2 = np.random.randn(n_samples)\n",
    "z3 = np.random.randn(n_samples) * 0.1\n",
    "\n",
    "X_5d = np.column_stack([\n",
    "    z1 + z2 + np.random.randn(n_samples) * 0.1,  # Highly correlated\n",
    "    z1 - z2 + np.random.randn(n_samples) * 0.1,  # Highly correlated\n",
    "    z1 * 0.5 + np.random.randn(n_samples) * 0.3,  # Moderately correlated\n",
    "    z3,  # Low variance\n",
    "    np.random.randn(n_samples) * 0.05  # Very low variance\n",
    "])\n",
    "\n",
    "# Fit PCA with all components\n",
    "pca_all = PCAAnomaly(n_components=5)\n",
    "pca_all.fit(X_5d)\n",
    "\n",
    "print(f\"\\nExplained variance by each component:\")\n",
    "for i, var in enumerate(pca_all.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "cumsum = np.cumsum(pca_all.explained_variance_ratio_)\n",
    "print(f\"\\nCumulative explained variance:\")\n",
    "for i, var in enumerate(cumsum):\n",
    "    print(f\"  First {i+1} components: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "# Plot explained variance\n",
    "fig = pca_all.plot_explained_variance()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: PCA - Mahalanobis Distance\n",
    "print(\"\\nTest 6: PCA - Mahalanobis Distance Method\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate elongated cluster\n",
    "np.random.seed(42)\n",
    "X_elongated = np.random.randn(100, 2)\n",
    "X_elongated[:, 0] *= 3  # Stretch along x-axis\n",
    "\n",
    "# Add outliers\n",
    "X_outliers_maha = np.array([[8, 8], [-8, -7]])\n",
    "X_maha = np.vstack([X_elongated, X_outliers_maha])\n",
    "\n",
    "# Fit PCA with Mahalanobis\n",
    "pca_maha = PCAAnomaly(n_components=2, method='mahalanobis', contamination=0.05)\n",
    "pca_maha.fit(X_maha)\n",
    "\n",
    "distances = pca_maha.mahalanobis_distance(X_maha)\n",
    "labels = pca_maha.predict(X_maha)\n",
    "\n",
    "print(f\"\\nMahalanobis distances:\")\n",
    "print(f\"  Mean (inliers): {np.mean(distances[:100]):.3f}\")\n",
    "print(f\"  Outliers: {distances[100:]}\")\n",
    "print(f\"  Threshold: {-pca_maha.threshold_:.3f}\")\n",
    "print(f\"  Detected outliers: {np.sum(labels)}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Mahalanobis distances\n",
    "scatter1 = axes[0].scatter(X_maha[:, 0], X_maha[:, 1], c=distances, s=80, \n",
    "                          cmap='RdYlGn_r', edgecolors='black', linewidths=1)\n",
    "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0].set_title('Mahalanobis Distance', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Distance')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Detection results\n",
    "axes[1].scatter(X_maha[labels == 0, 0], X_maha[labels == 0, 1], \n",
    "               c='blue', s=50, alpha=0.6, label='Inliers', edgecolors='black', linewidths=0.5)\n",
    "axes[1].scatter(X_maha[labels == 1, 0], X_maha[labels == 1, 1], \n",
    "               c='red', s=200, marker='*', label='Detected Outliers', \n",
    "               edgecolors='black', linewidths=1)\n",
    "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[1].set_title('Detection Results', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Porównanie LOF vs PCA\n",
    "\n",
    "Sprawdzmy, jak obie metody radzą sobie z tym samym zbiorem danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7: Porównanie LOF vs PCA\n",
    "print(\"\\nTest 7: Porównanie LOF vs PCA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(150, 2) * 0.8\n",
    "X_test_outliers = np.array([\n",
    "    [4, 4],\n",
    "    [-4, 3],\n",
    "    [3, -4],\n",
    "    [-3, -4],\n",
    "    [0, 5]\n",
    "])\n",
    "X_test_combined = np.vstack([X_test, X_test_outliers])\n",
    "y_test_true = np.hstack([np.zeros(150), np.ones(5)])\n",
    "\n",
    "# LOF\n",
    "lof_comp = LOF(n_neighbors=15)\n",
    "lof_scores = lof_comp.fit_predict(X_test_combined)\n",
    "lof_labels = (lof_scores > 1.5).astype(int)\n",
    "\n",
    "# PCA\n",
    "pca_comp = PCAAnomaly(n_components=1, method='reconstruction', contamination=0.05)\n",
    "pca_comp.fit(X_test_combined)\n",
    "pca_errors = pca_comp.reconstruction_error(X_test_combined)\n",
    "pca_labels = pca_comp.predict(X_test_combined)\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "lof_precision = precision_score(y_test_true, lof_labels, zero_division=0)\n",
    "lof_recall = recall_score(y_test_true, lof_labels)\n",
    "lof_f1 = f1_score(y_test_true, lof_labels)\n",
    "\n",
    "pca_precision = precision_score(y_test_true, pca_labels, zero_division=0)\n",
    "pca_recall = recall_score(y_test_true, pca_labels)\n",
    "pca_f1 = f1_score(y_test_true, pca_labels)\n",
    "\n",
    "print(f\"\\nLOF Results:\")\n",
    "print(f\"  Precision: {lof_precision:.3f}\")\n",
    "print(f\"  Recall: {lof_recall:.3f}\")\n",
    "print(f\"  F1-score: {lof_f1:.3f}\")\n",
    "\n",
    "print(f\"\\nPCA Results:\")\n",
    "print(f\"  Precision: {pca_precision:.3f}\")\n",
    "print(f\"  Recall: {pca_recall:.3f}\")\n",
    "print(f\"  F1-score: {pca_f1:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Plot 1: Ground truth\n",
    "axes[0].scatter(X_test_combined[y_test_true == 0, 0], \n",
    "               X_test_combined[y_test_true == 0, 1], \n",
    "               c='blue', s=50, alpha=0.6, label='True Inliers', \n",
    "               edgecolors='black', linewidths=0.5)\n",
    "axes[0].scatter(X_test_combined[y_test_true == 1, 0], \n",
    "               X_test_combined[y_test_true == 1, 1], \n",
    "               c='red', s=200, marker='*', label='True Outliers', \n",
    "               edgecolors='black', linewidths=1)\n",
    "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0].set_title('Ground Truth', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: LOF results\n",
    "scatter2 = axes[1].scatter(X_test_combined[:, 0], X_test_combined[:, 1], \n",
    "                          c=lof_scores, s=80, cmap='RdYlGn_r', \n",
    "                          edgecolors='black', linewidths=1)\n",
    "axes[1].scatter(X_test_combined[lof_labels == 1, 0], \n",
    "               X_test_combined[lof_labels == 1, 1], \n",
    "               facecolors='none', s=300, marker='o', \n",
    "               edgecolors='orange', linewidths=3, label='Detected')\n",
    "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[1].set_title(f'LOF (F1={lof_f1:.3f})', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='LOF Score')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: PCA results\n",
    "scatter3 = axes[2].scatter(X_test_combined[:, 0], X_test_combined[:, 1], \n",
    "                          c=pca_errors, s=80, cmap='RdYlGn_r', \n",
    "                          edgecolors='black', linewidths=1)\n",
    "axes[2].scatter(X_test_combined[pca_labels == 1, 0], \n",
    "               X_test_combined[pca_labels == 1, 1], \n",
    "               facecolors='none', s=300, marker='o', \n",
    "               edgecolors='orange', linewidths=3, label='Detected')\n",
    "axes[2].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[2].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[2].set_title(f'PCA (F1={pca_f1:.3f})', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter3, ax=axes[2], label='Reconstruction Error')\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Podsumowanie\n",
    "\n",
    "### 4.1 LOF (Local Outlier Factor)\n",
    "\n",
    "**Zalety**:\n",
    "- Wykrywa anomalie lokalne\n",
    "- Odporny na różne gęstości w zbiorze\n",
    "- Nie wymaga założeń o rozkładzie danych\n",
    "\n",
    "**Wady**:\n",
    "- Wysoka złożoność obliczeniowa O(n²) lub O(n log n) z KD-Tree\n",
    "- Wrażliwy na wybór parametru k\n",
    "- Może mieć problemy z danymi wysokowymiarowymi\n",
    "\n",
    "### 4.2 PCA (Principal Component Analysis)\n",
    "\n",
    "**Zalety**:\n",
    "- Redukcja wymiarowości\n",
    "- Interpretowalna transformacja\n",
    "- Skuteczna dla danych z liniowymi zależnościami\n",
    "- Niższa złożoność: O(nd² + d³)\n",
    "\n",
    "**Wady**:\n",
    "- Zakłada liniowość\n",
    "- Wrażliwa na skalę danych\n",
    "- Może przeoczyć nieliniowe anomalie\n",
    "- Może nie wykryć anomalii w kierunkach niskiej wariancji\n",
    "\n",
    "### 4.3 Kiedy używać której metody?\n",
    "\n",
    "**LOF**:\n",
    "- Dane z różnymi gęstościami\n",
    "- Anomalie lokalne\n",
    "- Niewielkie zbiory danych (< 10k próbek)\n",
    "\n",
    "**PCA**:\n",
    "- Dane wysokowymiarowe wymagające redukcji\n",
    "- Liniowe zależności między cechami\n",
    "- Duże zbiory danych\n",
    "- Anomalie w kierunkach głównych wariancji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RAPORT 2: Implementacja Podstawowa - ZAKOŃCZONY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n✓ Implementacja LOF - DONE\")\n",
    "print(\"✓ Implementacja PCA - DONE\")\n",
    "print(\"✓ Testy jednostkowe - DONE\")\n",
    "print(\"✓ Demonstracja algorytmów - DONE\")\n",
    "print(\"\\nNastępny krok: Raport 3 - Isolation Forest i optymalizacje\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
